DEMO: How to run and test the Navigate AI demo

Prerequisites:
- Node.js 18+ and npm installed
- Git and a GitHub account (for Vercel deploy)

1) Install dependencies

Open a terminal and run:

  cd relocate-ai
  npm install

In a second terminal (for the API):

  cd api
  npm install

2) Run locally (mock LLM responses by default)

Start the API (provides /llm, /bookings, /payments):

  cd api
  npm start

Start the React app (dev server with proxy to the API):

  cd relocate-ai
  npm start

Open the app in your browser at http://localhost:3000

Quick demo actions to try in the UI:
- Ask: "visa from Lagos to USA" — returns a simulated visa guide
- Ask: "book flight" or "find flights" — returns simulated flight options
- Try booking flows to see reservation/payment mocks

3) Environment variables
- To enable a real GROQ/OpenAI provider, set the following in `api/.env` or in your deployment environment:

  GROQ_API_KEY=your_api_key_here
  GROQ_API_URL=https://api.groq.com/openai/v1/chat/completions

- If `GROQ_API_KEY` is not set, the API will return simulated responses (safe for demos).

4) Build for production

  cd relocate-ai
  npm run build

This produces a `build/` folder (static site). Note: generated `build/` is excluded from the repo.

5) Manual deploy to Vercel (recommended for this repo)

- Push your branch to GitHub.
- In Vercel, import the repo and keep default settings. This repo contains `vercel.json` which:
  - Builds `relocate-ai` as a static site from `relocate-ai/package.json` (distDir: build)
  - Deploys files in `api/` as Node serverless functions

- In the Vercel project settings, add environment variables (Secrets):
  - `GROQ_API_KEY` (if you want real LLM responses)
  - `GROQ_API_URL` (optional override)

6) Notes & caveats
- The `proxy` field in `relocate-ai/package.json` is only for local dev. Production requests must point to `/api/*` endpoints provided by the deployed functions or to a separate API host.
- Current `api/server.js` is an Express server; on Vercel it will run as serverless functions but you can also split it into individual function files for better cold-start behavior.
- Serverless limits: long-running or streaming LLM calls may hit execution time limits. For heavy or streaming workloads, consider deploying a dedicated server.

7) Troubleshooting
- If the web app cannot reach `/api/llm`, ensure the API is running locally and the dev server shows the proxy is active.
- Check `api` logs for errors: run `cd api && npm start` and watch console output.

If you want, I can:
- Convert `api/server.js` into separate Vercel serverless function files (`api/llm.js`, `api/bookings.js`, `api/payments.js`).
- Add a short README section with these commands.

Enjoy the demo!
